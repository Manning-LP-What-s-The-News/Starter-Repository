{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer,one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"/Users/s0c02nj/Downloads/cnn/dataframe_extractive.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['story_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stories_subset = list(data['story_id'].sample(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = data[data['story_id'].isin(list_stories_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence_Length Max Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_sent_count = data_sub.groupby('story_id').size().reset_index(name='count_sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(d1_sent_count['count_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_sent_count['count_sentences'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = d1_sent_count['count_sentences'].max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ls = set(data_sub['sentence'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(set(data_sub[\"label_sent\"].values))\n",
    "n_tags = len(tags); n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Dictionary and Inverse\n",
    "sent2idx = {s: i + 2 for i, s in enumerate(sent_ls)}\n",
    "sent2idx[\"UNK\"] = 1\n",
    "sent2idx[\"PAD\"] = 0\n",
    "\n",
    "idx2word = {i: s for s, i in sent2idx.items()}\n",
    "\n",
    "tag2idx  = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 3\n",
    "idx2tag = {i: s for s, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Distribution and Max Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub['word_len'] = data_sub['sentence'].progress_apply(lambda x: len(x.split()))\n",
    "data_sub['word_lis'] = data_sub['sentence'].progress_apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data_sub['word_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub['word_len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Word_level_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ls = set(list(chain(*data_sub['word_lis'].tolist())))\n",
    "n_words = len(word_ls)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {c: i+2 for i,c in enumerate(word_ls)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_word = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting the Data for Modelling Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub['sent_lab'] = data_sub[['sentence','label_sent']].apply(tuple, axis=1)\n",
    "data_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_labels = []\n",
    "\n",
    "for s_id in tqdm(list_stories_subset):\n",
    "    \n",
    "    #Initialising the list\n",
    "    temp_story = []\n",
    "    \n",
    "    #Storing the temo\n",
    "    temp_story = list(data_sub[data_sub['story_id'] == s_id]['sent_lab'])\n",
    "    \n",
    "    #Appending the final list in the sent label format\n",
    "    stories_labels.append(temp_story)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling Excersie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the word encoding\n",
    "\n",
    "X_word = []\n",
    "for story in tqdm(stories_labels):\n",
    "    \n",
    "    #Give the seq\n",
    "    story_seq = []\n",
    "    \n",
    "    #To give an upper bound on the maximum length of the word sequence for sentence\n",
    "    for i in range(max_len):\n",
    "        sent_seq = []\n",
    "        \n",
    "        #to give an upper bound on the maximum length of words to consider\n",
    "        for j in range(max_len_word):\n",
    "            try:\n",
    "                split_sent = story[i][0].split()\n",
    "                sent_seq.append(word2idx.get(split_sent[j]))\n",
    "            except:  \n",
    "                #exception will be there when there will not be any sentence for the length and will be padded 0\n",
    "                sent_seq.append(word2idx.get(\"PAD\"))\n",
    "        story_seq.append(sent_seq)\n",
    "    X_word.append(np.array(story_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_labels[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx.get(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[tag2idx[w[1]] for w in s] for s in stories_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pad_sequences(maxlen = max_len, \n",
    "                  sequences = y, \n",
    "                  value = tag2idx[\"PAD\"], \n",
    "                  padding = 'post', \n",
    "                  truncating = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_word_tr, X_word_te, _, _ = train_test_split(X_word, y, \n",
    "                                              test_size=0.2, \n",
    "                                              random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "\n",
    "+ output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "\n",
    "\n",
    "+ input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input and embeddings for words\n",
    "word_in = Input(shape=(max_len, max_len_word,))\n",
    "\n",
    "#Word_level embedding\n",
    "emb_word = TimeDistributed(Embedding(input_dim = n_words +2, output_dim = 50,\n",
    "                           input_length = max_len_word, mask_zero=True))(word_in)\n",
    "\n",
    "\n",
    "# #word LSTM to get sent encodings by words\n",
    "emb_sent = TimeDistributed(LSTM(units = 32, return_sequences=False,\n",
    "                                recurrent_dropout=0.3))(emb_word)\n",
    "\n",
    "# #main LSTM\n",
    "# x = concatenate([emb_word, char_enc])\n",
    "\n",
    "#x = SpatialDropout1D(0.3)(x)\n",
    "main_lstm = Bidirectional(LSTM(units=64, return_sequences=True,\n",
    "                               recurrent_dropout=0.3))(emb_sent)\n",
    "\n",
    "out = TimeDistributed(Dense(n_tags + 1, activation=\"softmax\"))(main_lstm)\n",
    "\n",
    "\n",
    "model = Model([word_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([np.array(X_word).reshape((len(X_word), max_len, max_len_word))],\n",
    "                     np.array(y).reshape(len(y), max_len, 1),\n",
    "                     batch_size=512, \n",
    "                     epochs=1, \n",
    "                     validation_split=0.1, \n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
